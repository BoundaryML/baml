---
title: Switching LLMs
---

Switch LLMs using the `client` property. You can use the shorthand form, or the longer form with a named client.

The shorthand form is `<provider>/<model>`:

which uses ANTHROPIC_API_KEY or OPENAI_API_KEY environment variables as the defaults.

```rust BAML
function MakeHaiku(topic: string) -> string {
  client "openai/gpt-4o" // or anthropic/claude-3-5-sonnet-20240620
  prompt #"
    Write a haiku about {{ topic }}.
  "#
}
```

The longer form uses a named client, and supports adding any parameters supported by the provider or changing the temperature, top_p, etc.

```rust BAML
client<llm> MyClient {
  provider "openai"
  options {
    model "gpt-4o"
    api_key env.OPENAI_API_KEY
    // other params like temperature, top_p, etc.
    temperature 0.5
    base_url "https://my-custom-endpoint.com/v1"
    // add headers
    headers {
      "anthropic-beta" "prompt-caching-2024-07-31"
    }
  }

}

function MakeHaiku(topic: string) -> string {
  client MyClient
  prompt #"
    Write a haiku about {{ topic }}.
  "#
}
```

Consult the [provider documentation](#fields) for a list of supported providers
and models, the default options, and setting [retry policies](/docs/reference/retry-policy).

<Tip>
If you want to specify which client to use at runtime, in your Python/TS/Ruby code,
you can use the [client registry](/docs/calling-baml/client-registry) to do so.

This can come in handy if you're trying to, say, send 10% of your requests to a
different model.
</Tip>