# This file is generated by the BAML compiler.
# Do not edit this file directly.
# Instead, edit the BAML files and recompile.

# ruff: noqa: E501,F401
# flake8: noqa: E501,F401
# pylint: disable=unused-import,line-too-long
# fmt: off

from ..clients.client_azure_gpt4 import AZURE_GPT4
from ..functions.fx_maybepolishtext import BAMLMaybePolishText
from ..types.classes.cls_conversation import Conversation
from ..types.classes.cls_improvedresponse import ImprovedResponse
from ..types.classes.cls_message import Message
from ..types.classes.cls_proposedmessage import ProposedMessage
from ..types.enums.enm_messagesender import MessageSender
from ..types.enums.enm_sentiment import Sentiment
from baml_lib._impl.deserializer import Deserializer
from typing import Callable
from baml_lib._impl.functions import OnStreamCallable


# Impl: v1
# Client: AZURE_GPT4
# An implementation of .


__prompt_template = """\
Given a conversation with a resident, consider improving the response previously shown.

Good responses are amiable and direct.

Do not use or negative unless the question is a yes or no question.

```input
{arg}
```       


Output JSON Format:
{
  // false if the response is already contextual and pleasant
  "ShouldImprove": bool,
  // string if should_improve else null
  "improved_response": string | null,
  "field": "Sentiment as string"
}

JSON:\
"""

__input_replacers = {
    "{arg}"
}


# We ignore the type here because baml does some type magic to make this work
# for inline SpecialForms like Optional, Union, List.
__deserializer = Deserializer[ImprovedResponse](ImprovedResponse)  # type: ignore
__deserializer.overload("ImprovedResponse", {"ShouldImprove": "should_improve"})






async def v1(arg: ProposedMessage, /) -> ImprovedResponse:
    response = await AZURE_GPT4.run_prompt_template(template=__prompt_template, replacers=__input_replacers, params=dict(arg=arg))
    deserialized = __deserializer.from_string(response.generated)
    return deserialized


async def v1_stream(arg: ProposedMessage, /, __onstream__: OnStreamCallable) -> ImprovedResponse:
    # Since the original operation was commented out, we'll implement a placeholder
    # This should mimic the streaming operation, calling __onstream__ with a string
    
    # Placeholder: simulate streaming by calling __onstream__ with a mock response
    mock_stream_response = "Streaming response part"  # Mock response part for demonstration
    # __onstream__(mock_stream_response)  # Call the __onstream__ callback with mock data
    
    # Here you would have your actual streaming logic, something like:
    # while not end_of_stream:
    #     response_part = await get_next_stream_part(...)
    #     deserialized_part = __deserializer.from_string(response_part)
    #     __onstream__(deserialized_part)
    # return final_response

    # Placeholder for the final return, since the real implementation is commented out
    # In a real scenario, you would return the final or aggregated response from the stream
    return ImprovedResponse(should_improve=True, improved_response="Improved response", field=Sentiment.Negative) 

BAMLMaybePolishText.register_impl("v1")(v1, v1_stream)
