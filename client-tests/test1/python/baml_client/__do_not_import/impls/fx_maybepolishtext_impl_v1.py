# This file is generated by the BAML compiler.
# Do not edit this file directly.
# Instead, edit the BAML files and recompile.

# ruff: noqa: E501,F401
# flake8: noqa: E501,F401
# pylint: disable=unused-import,line-too-long
# fmt: off

from ..clients.client_azure_gpt4 import AZURE_GPT4
from ..functions.fx_maybepolishtext import BAMLMaybePolishText
from ..types.classes.cls_conversation import Conversation
from ..types.classes.cls_message import Message
from ..types.classes.cls_proposedmessage import ProposedMessage
from ..types.enums.enm_messagesender import MessageSender
from ..types.partial.classes.cls_conversation import PartialConversation
from ..types.partial.classes.cls_message import PartialMessage
from ..types.partial.classes.cls_proposedmessage import PartialProposedMessage
from baml_core.stream import AsyncStream
from baml_lib._impl.deserializer import Deserializer
from baml_core.provider_manager.llm_response import LLMResponse
from typing import AsyncIterator
import contextlib

import typing
# Impl: v1
# Client: AZURE_GPT4
# An implementation of MaybePolishText.

__prompt_template = """\
Write a haiku of the ocean:\
"""

__input_replacers = {
}


# We ignore the type here because baml does some type magic to make this work
# for inline SpecialForms like Optional, Union, List.
__deserializer = Deserializer[str](str)  # type: ignore
__deserializer.overload("ImprovedResponse", {"ShouldImprove": "should_improve"})

# Add a deserializer that handles stream responses, which are all Partial types
__partial_deserializer = Deserializer[str](str)  # type: ignore
__partial_deserializer.overload("ImprovedResponse", {"ShouldImprove": "should_improve"})







async def v1(arg: ProposedMessage, /) -> str:
    response = await AZURE_GPT4.run_prompt_template(template=__prompt_template, replacers=__input_replacers, params=dict(arg=arg))
    deserialized = __deserializer.from_string(response.generated)
    return deserialized




def v1_stream(arg: ProposedMessage, /) -> AsyncStream[str, str]:
    def run_prompt() -> AsyncIterator[LLMResponse]:
        print("running prompt")
        return AZURE_GPT4.run_prompt_template_stream(template=__prompt_template, replacers=__input_replacers, params=dict(arg=arg))
    stream = AsyncStream(stream_cb=run_prompt, partial_deserializer=__partial_deserializer, final_deserializer=__deserializer)
    return stream


BAMLMaybePolishText.register_impl("v1")(v1, v1_stream)
