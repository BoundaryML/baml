https://docs.vllm.ai/ - Easy, fast, and cheap LLM serving for everyone

VLLM supports the OpenAI client, allowing you to use the [openai](/docs/snippets/clients/providers/openai) provider with an overriden `base_url`


See https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html for more information.

```baml BAML
client<llm> MyClient {
  provider openai
  options {
    base_url "http://localhost:8000/v1"
    api_key "token-abc123"
    model "NousResearch/Meta-Llama-3-8B-Instruct"
    default_role "user" // Required for using VLLM
  }
}
```