---
title: client
---

A **client** is the mechanism by which a function calls an LLM.

## Syntax

```rust
client<CLIENT_TYPE> Name {
    provider ProviderName
    options {
      // ...
    }
}
```

- `CLIENT_TYPE`: What type of AI model will be used. Currently must be `llm`
- `Name`: The name of the client (can be any [a-zA-Z], numbers or `_`). Must start with a letter.

## Properties

| Property       | Type                 | Description                                        | Required |
| -------------- | -------------------- | -------------------------------------------------- | -------- |
| `provider`     | name of the provider | The provider to use.                               | Yes      |
| `options`      | key-value pair       | These are passed through directly to the provider. | No       |
| `retry_policy` | name of the policy   | [Learn more](/docs/syntax/client/retry)              | No       |

## Providers

BAML ships with the following providers (you can can also write your own!):

- LLM client providers
  - `openai`
  - `azure-openai`
  - `anthropic`
  - `google-ai`
  - `ollama`
- Composite client providers
  - `fallback`
  - `round-robin`

There are two primary types of LLM clients: chat and completion. BAML abstracts
away the differences between these two types of LLMs by putting that logic in
the clients.

You can call a chat client with a single completion prompt and it will
automatically map it to a chat prompt. Similarly you can call a completion
client with multiple chat prompts and it will automatically map it to a
completion prompt.

### OpenAI/Azure

Provider names:

- `openai-azure`

You must pick the right provider for the type of model you are using. For
example, if you are using a GPT-3 model, you must use a `chat` provider, but if
you're using the instruct model, you must use a `completion` provider.

You can see all models supported by OpenAI [here](https://platform.openai.com/docs/models).

Accepts any options as defined by [OpenAI/Azure SDK](https://github.com/openai/openai-python/blob/9e6e1a284eeb2c20c05a03831e5566a4e9eaba50/src/openai/types/chat/completion_create_params.py#L28)

See [Azure Docs](https://learn.microsoft.com/en-us/azure/ai-services/openai/quickstart?tabs=command-line,python&pivots=programming-language-python#create-a-new-python-application) to learn how to get your Azure API key.

```rust
// A client that uses the OpenAI chat API.
client<llm> MyGPT35Client {
  // Since we're using a GPT-3 model, we must use a chat provider.
  provider openai
  options {
    model gpt-3.5-turbo
    // Set the api_key parameter to the OPENAI_API_KEY environment variable
    api_key env.OPENAI_API_KEY
  }
}

// A client that uses the OpenAI chat API.
client<llm> MyAzureClient {
  // I configured the deployment to use a GPT-3 model,
  // so I must use a chat provider.
  provider openai-azure
  options {
        api_key env.AZURE_OPENAI_KEY
        // This may change in the future
        api_version "2023-05-15"
        api_type azure
        azure_endpoint env.AZURE_OPENAI_ENDPOINT
        model "gpt-35-turbo-default"
    }
}
```


### Anthropic

Provider names:

- `anthropic`

Accepts any options as defined by [Anthropic SDK](https://github.com/anthropics/anthropic-sdk-python/blob/fc90c357176b67cfe3a8152bbbf07df0f12ce27c/src/anthropic/types/completion_create_params.py#L20)

```rust
client<llm> MyClient {
  provider baml-anthropic
  options {
    model claude-2
    max_tokens_to_sample 300
  }
}
```
### Google 

Provider names:
- `google-ai`

Accepts any options as defined by the [Gemini SDK](https://ai.google.dev/gemini-api/docs/get-started/tutorial?lang=rest#configuration).

```rust
client<llm> MyGoogleClient {
  provider google-ai
  options{
    model "gemini-1.5-pro-001"
  }
}
```

### Ollama

- BAML Python Client >= 0.18.0
- BAML Typescript Client >= 0.0.6

Provider names:

- `ollama`

Accepts any options as defined by [Ollama SDK](https://github.com/ollama/ollama/blob/main/docs/api.md#generate-a-chat-completion).

```rust
client<llm> MyOllamaClient {
  provider ollama
  options {
    model llama2
  }
}
```
#### Requirements

1. For Ollama, in your terminal run `ollama serve`
2. In another window, run `ollama run llama2` (or your model), and you should be good to go.
3. If your Ollama port is not 11434, you can specify the endpoint manually.

```rust
client<llm> MyClient {
  provider ollama
  options {
    model llama2
    options {
      temperature 0
      base_url "http://localhost:<ollama_port>" // Default is 11434
    }
  }
}
```


This is not the Vertex AI Gemini API, but the Google Generative AI Gemini API, which supports the same models but at a different endpoint.


### Fallback

The `baml-fallback` provider allows you to define a resilient client, by
specifying strategies for re-running failed requests. See
[Fallbacks/Redundancy](/docs/syntax/client/redundancy) for more information.

### Round Robin

The `baml-round-robin` provider allows you to load-balance your requests across
multiple clients. Here's an example:

```rust
client<llm> MyClient {
  provider round-robin
  options {
    strategy [
      MyGptClient
      MyAnthropicClient
    ]
  }
}
```

This will alternate between `MyGptClient` and `MyAnthropicClient` for successive
requests, starting from a randomly selected client (so that if you run multiple
instances of your application, they won't all start with the same client).

If you want to control which client is used for the first request, you can specify
a `start` index, which tells BAML to start with the client at index `start`, like
so:

```rust
client<llm> MyClient {
  provider baml-round-robin
  options {
    start 1
    strategy [
      MyGptClient
      MyAnthropicClient
    ]
  }
}
```

## Other providers
You can use the `openai` provider if the provider you're trying to use has the same ChatML response format (i.e. HuggingFace via their Inference Endpoint or your own local endpoint)

Some providers ask you to add a `base_url`, which you can do like this:
  
```rust
client<llm> MyClient {
  provider openai
  options {
    model some-custom-model
    api_key env.OPEN
    base_url "https://some-custom-url-here"
  }
}
```