---
title: "Level 1: Writing AI functions"
---

### Pre-requisites
- Follow the [installation](/v3/home/installation) instructions to ensure you have a BAML environment set up.

## Overview

Before you call an LLM, ask yourself what kind of input or output youre expecting. If you want the LLM to generate text, then you probably want a string, but if you're trying to get it to collect user details, you may want it to return a complex type like a `UserInfo` object.

Thinking this way can help you decompose large complex prompts into smaller, more measurable functions, and will also help you build more complex workflows and agents.

To get us started using BAML, we'll start with writing a simple AI function that extracts verbs from a sentence and start adding more and more complexity in later tutorials.


## The BAML Project structure
At a high level, you will define your AI prompts/interfaces in BAML files, which will then compile into generated Python (soon TS) code that you can import and use in your codebase.

Here is the typical project structure:

```bash
.
├── baml_client/ # Generated code
├── baml_src/ # Prompts live here
│   ├── __tests__/ # Tests loaded by playground
│   │   ├── YourAIFunction/
│   │   │   ├── test_name_monkey.json
│   │   │   └── test_name_cricket.json
│   │   └── YourAIFunction2/
│   │       └── test_name_jellyfish.json
│   ├── main.baml
│   ├── any_directory/
│   │   ├── bar.baml
│   │   └── baz.baml
│   └── foo.baml
# The rest of your project (not generated / used by BAML)
├── app/
│  ├── __init__.py
│  └── main.py
├── pyproject.toml
└── poetry.lock

```

1. **baml_src** is the directory where you write your BAML files with the AI function declarations, prompts, retry policies, etc. It also contains [generator](/v3/syntax/generator) blocks which configure how and where to transpile your BAML code.
2. **baml_client** is the directory where the generated python or TS client code lives. This is the code that you import into your python or TS program (more on this later)

<Warning>
  **You should never edit any files inside baml_client directory** as the whole
  directory gets regenerated everytime you save a .baml file in VSCode. VScode runs `baml build` under the hood.
</Warning>

<Tip>
  If you ever run into any issues with the generated code (like merge
  conflicts), you can always delete the `baml_client` directory and it will get
  regenerated automatically once you fix any other conflicts in your `.baml`
  files.
</Tip>


# Implementing an AI function
## 1. Define AI functions and models in BAML files
First we will define a function of the following signature in BAML:
`ExtractVerbs(title: string, body: string) -> string[]`

Here's the BAML equivalent:
```rust
/// Takes a title and body, and returns a list of verbs
function ExtractVerbs {
    input (title: string, body: string)
    /// list of verbs
    output string[]
}
```
Every BAML function has a strictly typed input and output. The input and output can be either a **primitive type** (string, number, boolean) or a [complex type](/v3/syntax/type) (think unions, lists, or even custom pydantic models)

To ensure the baml compiler generates your Python / TS code, you'll also need to add a `generator` block to your main.baml file:

```rust
generator {
  language python
    // poetry is the default if this doesn't exist
  pkg_manager pip
}
```

<Tip>In baml files there are no imports. All BAML entities are available to all other files under the same parent `baml_src` directory. </Tip>

## 2. Implement the function using an LLM

To implement the function we need two things:
1. An LLM client that defines which LLM to call and with which params.
2. The actual prompt.

<Steps>
<Step title="Define the LLM client">
To implement a client we can just define one like this in a BAML file. Learn more about [clients](/v3/syntax/client) and non-openai chat providers.
```rust
client<llm> GPT4 {
  provider baml-openai-chat
  options {
    model gpt-4 
    api_key env.OPENAI_API_KEY
  }
}
```
<Tip>
Use any parameters available to that model, like temperature etc, by adding them to the options block. You can also use environment variables to store secrets like API keys.
</Tip>
</Step>
<Step title="Define a prompt">
Next we can create the prompt by **implementing** the function using an LLM.
In BAML we provide helper utilities to inject the input variables into the prompt, and also get the LLM to return the right output type. You always get full-view of the whole prompt string, without any magic.

```rust
impl<llm, ExtractVerbs> version1 {
  client GPT4
  prompt #"
    Extract the verbs from this paragraph:
    
    Title: {#input.title}
    ---
    {#input.body}
    --
    {// this is a comment inside a prompt! //}
    Return a {#print_type(output)}.

    Response:
  "#
}
```

In VSCode it looks like this:
<img src="/images/v3/extractverbs_playground.png"/>

In here you'll notice how our language automatically dedents strings, injects variables into the prompt, and supports comments that will be stripped from the actual prompt. See our [syntax guide](/v3/syntax/strings) for more information on basic string / comment syntax. 

</Step>
</Steps>

## 3. Use the function in your Python / TS code
Our VSCode extension automatically generates a python **baml_client** to access and call your functions.
```python
from baml_client import baml as b

verb_list = await b.ExtractVerbs(
    title="Hello", body="This is a paragraph"
)

if len(verb_list) == 1:
    print("There is 1 verb in this paragraph")
else:
    print(f"There are {len(verb_list)} verb(s) in this paragraph")
```

## 4. [Recommended] Write unit tests


As you keep working with BAML, we recommend [setting up a test suite](/v3/home/running-tests) or using our [tracing capabilities](/v3/home/tracing-tagging) to make it easier to dive into issues (and get some nice analytics in Boundary Studio).


Continue to the next guides for more details on how to write AI functions and models, and learn to use the BAML syntax one step at a time.

