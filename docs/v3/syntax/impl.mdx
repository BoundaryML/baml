---
title: "impl"
---
<Warning>
    You are currently viewing the old BAML documentation. This syntax is still supported, but will eventually be deprecated in newer versions of BAML.
</Warning>

An **impl** is an implementation of a `function` using an AI model (LLM or otherwise).

## Syntax

```rust
impl<IMPL_TYPE, FUNCTION_NAME> Name {
    client SomeClient
}
```

- `IMPL_TYPE`: What type of AI model will be used. Currently must be `llm`
- `FUNCTION_NAME`: The name of the function this is an implementation of
- `Name`: The name of the impl (can be any [a-zA-Z], numbers or `_`)

## Properties

Depending on the `IMPL_TYPE`, additional properties may exist.

| Property  | Type                                         | Description                                                                                                                                                                                 | Required |
| --------- | -------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------- |
| `client`  | string                                       | The name of the client this impl will use. The type of the client must match the type of the impl (both llm). See [Client](/v3/syntax/client)                                               | YES      |
| `adapter` | See [Adapters](/v3/syntax/function-adapters) | Modify a type temporarily for the use in the impl without exposing it to the buisness logic. e.g. force an llm to extract extra fields not specified earlier. **This is an advanced tool!** | NO       |

### impl types

#### ✅ llm

`llm` models are those which take a prompt (string) and return a string. For example, GPT-3, GPT-J, etc.
They may also return some other metadata (e.g. logprobs, stop_reason, etc.)

| Property | Type   | Description                                                                                                                                           | Required |
| -------- | ------ | ----------------------------------------------------------------------------------------------------------------------------------------------------- | -------- |
| `prompt` | string | The string passed into the LLM. You can compose this this variables and other template types. See [Prompt Engineering](/v3/syntax/prompt-engineering) | YES      |

#### ❌ probability

Not supported yet.

`probability` models are those which take in any type, and return `float[]` of a fixed length. For example, BERT, etc.And yes we know technically LLMs do this under the hood, but most practical LLMs outputs are synthesized into a string.

## Examples

### Sentiment Analysis

<Steps>
<Step title="Defining in baml">

<Info>
`{#input}` and `{#print_enum(Sentiment)}` are template variables injected into the prompt. See [Prompt Engineering](/v3/syntax/prompt-engineering) for more details.
</Info>

```rust baml_src/my_file.baml
impl<llm, GetSentiment> my_version {
    client GPT35Client
    prompt #"
        Given a sentence, return the sentiment.
        ---
        {#input}
        ---

        {#print_enum(Sentiment)}

        Sentiment:
    "#

}

// Define the function
function GetSentiment {
    input string
    output Sentiment
}

// Define the enum for sentiment
enum Sentiment {
    Positive
    Negative
    Neutral
}

// Use OpenAI's GPT-3.5 turbo model
client<llm> GPT35Client {
    provider baml-openai-chat
    options {
        model gpt-3.5-turbo
    }
}
```

</Step>
<Step title="Using the generated code">

<CodeGroup>

```python Python
from baml_client import baml as b
from baml_client.types import Sentiment

async def pipeline(arg: str) -> str:
    sentiment = await b.GetSentiment(arg)

    if sentiment == Sentiment.Positive:
        return "I'm glad you're having a good day!"
    elif sentiment == Sentiment.Negative:
        return "I'm sorry to hear that."
    else:
        return "I see it's just a normal day for you."

```

```typescript TypeScript
import b from "@/baml_client";
import { Sentiment } from '@/baml_client/types'

const pipeline = async (arg: string): string => {
  const category = await b.GetSentiment(arg);
  if (sentiment === Sentiment.Positive) {
    return "I'm glad you're having a good day!"
  } else if (sentiment === Sentiment.Negative) {
    return "I'm sorry to hear that."
  } else {
    return "I see it's just a normal day for you."
  }
};
```

</CodeGroup>
</Step>

<Step title="Calling a specific implementation of a function">

If you provide multiple implementations of a given function in BAML (e.g. `impl<llm, Fn> my_version` and `impl<llm, Fn> dev_version`), you can specify which should be used by default:

```rust BAML
function GetSentiment {
    input string
    output Sentiment
    default_impl my_version
}
```

Alternatively, you can also directly call a specific implementation:

<CodeGroup>
```python Python
await b.GetSentiment.get_impl('my_version').run(arg)
```

```typescript TypeScript
await b.GetSentiment.getImpl('my_version').run(arg)
```
</CodeGroup>

</Step>
</Steps>
