---
title: "ðŸ”¨ As an AI Engineer"
---

To reduce context switching AI engineers need to make when building AI pipelines, Boundary provides the following tools:

**VSCode BAML Playground** - LLM options like model, retry, caching, etc. are configurable. Test using the UI, or programmatically via our pytest plugin. As you edit .baml files, the playground autoupdates. Here's a demo video of the VSCode Playground in action:


<iframe
  src="https://player.cloudinary.com/embed/?public_id=baml-playground&cloud_name=dn7wj4mr5"
  width="600"
  height="400"
  allow="autoplay; fullscreen; encrypted-media; picture-in-picture"
  allowFullScreen
></iframe>

**Special String Syntax**: comments in prompts, prompt linting, realtime preview of the full prompt (no abstraction layers) and [more](https://docs.boundaryml.com/v2/mdx/overview#why-use-baml-vs-other-libraries-or-frameworks)

<img src="/images/v3/extractverbs_playground.png"/>


**[A super-powered deserializer](/v3/syntax/prompt_engineering/type-deserializer)**: Our generated SDK converts LLM output strings into your desired output type (significantly more powerful than json.parse, pydantic, zod, or Marvin). It supports complex models (see [Supported Types](https://docs.boundaryml.com/v2/mdx/ref/type)) like custom data models or even Unions in case your LLM has to choose between outputting two different schemas. Focus on the prompt instead of how to the parse the right output. We do this without any LLM API calls - no retries, no latency.

<img src="/images/v3/BAML_deserializer_2.png" />

