---
title: What is Boundary?
"og:description": Boundary is the test-driven toolkit for AI engineers.
"og:image": https://mintlify.s3-us-west-1.amazonaws.com/gloo/images/v3/AITeam.png
"twitter:image": https://mintlify.s3-us-west-1.amazonaws.com/gloo/images/v3/AITeam.png
---

Most AI engineers use LLMs to get structured outputs (e.g. a json schema) from unstructured inputs (strings). For example, to extract a resume from a chunk of text.

Existing LLM python libraries aren't powerful enough nor can they support easy testing capabilities (see our comparisons section) -- so we decided to build a compiler.

## Introducing BAML + The first VSCode LLM Playground

**BAML** (Basically, A Made-Up Language) is a lightweight programming language to define AI functions with structured inputs and outputs using natural language.


BAML comes with a **VSCode Playground**, which allows you to test prompts instantly with any LLM, without ever leaving VSCode.

<img src="/images/v3/testing_2.gif" />

Here's a 1-min video on how **BAML works seamlessly with Python (or TypeScript)** to get structured outputs from LLMs:  

<iframe width="560" height="315" src="https://www.youtube.com/embed/dpEvGrVJJng?si=6CPRTxil8WjQ_t5w" title="YouTube video player" frameBorder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowFullScreen></iframe>

Here’s a `.baml` AI function:

```rust example.baml
// example.baml
class Resume {
  name string
  skills string[]
}

function ExtractResume {
  input (resume_text: string)
  output Resume[]
}

impl<llm, ExtractResume> version1 {
  client GPT4Client // client definition not shown
  prompt #"
    Parse the following resume and return a structured representation of the data in the schema below.

    Resume:
    ---
    {#input.resume_text}
    ---
    Output in this JSON format:
    {#print_type(output)}

    Output JSON:
  "#
}
```

**BAML compiles to fully typed Python and TypeScript**. No matter how you change the prompt, or the LLM model, or fail-overs, the python code doesn’t change — unless you change your AI function’s signature.

The BAML compiler generates all the parsing boilerplate you need. No need to parse with `json.loads` ever again.

```python app.py
from baml_client import baml as b

async def main():
  resume = await b.ExtractResume(resume_text="""John Doe
Python, Rust
University of California, Berkeley, B.S. 
in Computer Science, 2020""")

  assert resume.name == "John Doe"
```

BAML can be deployed to any container, with only a single package dependency required (`e.g. pip install baml`). 
{/* [Learn more](/v3/home/deployment) */}

<Frame caption="BAML VSCode Playground">
<img src="/images/v3/baml_playground.png" />
</Frame>

## Getting Started

Start by [installing BAML](/v3/home/installation) and reading our [Hello World Tutorial](/v3/guides/hello_world/level0). 

Learning a new language may seem daunting, but it takes < 10 minutes to get started. 

The VSCode extension provides auto-compiling on save, a realtime preview of the full prompt, syntax highlighting and great errors — every syntax error recommends a fix. 

Making BAML easy to read and write is our core design philosophy.

#### What you get out-of-the-box
- **Typed Python/Typescript support**
- **VSCode Playground** -- see the full prompt and run tests
- **Better code organization** — no scattered jinja templates or yaml files
- **Its fast!**  -- BAML compiles into PY and TS in less than 50ms (We ❤️ Rust)
- **Full integration with** **Boundary Studio** -- our observability dashboard
    - Turn live production data into a test-case with one click!
- **Get structured responses,** 11 natively supported  types, including custom classes
- **Hallucination Checks** -- when LLMs return something unexpected, we throw an exception
- **Works with any LLM,** even your own

- And best of all, **everything lives in your codebase.**

    