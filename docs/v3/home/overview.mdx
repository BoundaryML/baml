---
title: What is Boundary?
"og:description": BoundaryML is a toolchain built in rust for building, testing, monitoring, and improving AI applications. Imagine trusting people to run tests instead of running it on CI/CD. It works, until it doesn't. ML systems are the same. Our toolchain is a language called **BAML**, a **VSCode extension** and a **dashboard** that all work together.
"og:image": https://mintlify.s3-us-west-1.amazonaws.com/gloo/images/v3/AITeam.png
"twitter:image": https://mintlify.s3-us-west-1.amazonaws.com/gloo/images/v3/AITeam.png
---

The Boundary Toolchain is a suite of tools that enable test-driven AI development using strongly typed function interfaces.

(To skip the background and see the code, [click here](/v3/home/overview#introducing-baml-the-first-vscode-llm-playground))

## Our Inspiration

The first problem LLM developers had to solve was strings. In the LLM world, everything is a string and that sucks.

Strongly-typed systems are more robust and easier to maintain.

For example, Microsoft created TypeChat (7.1k stars) to get structured outputs out of LLMs. [See example](https://github.com/microsoft/TypeChat/blob/main/examples/sentiment/src/main.ts). 

A python framework, [Marvin](https://github.com/PrefectHQ/marvin) (4.2k stars), also helped developers declare structured AI interfaces using their `@ai_fn` decorator. Under the hood, it calls openai for you. It's really elegant!

```python
from typing_extensions import TypedDict
from marvin import ai_fn

class DetailedSentiment(TypedDict):
    """A detailed sentiment analysis result.

    - `sentiment_score` is a number between 1 (positive) and -1 (negative)
    - `summary_in_a_word` is a one-word summary of the general sentiment
    """
    sentiment_score: float
    summary_in_a_word: str

@ai_fn
def get_detailed_sentiment(text: str) -> DetailedSentiment:
    """What do you think the sentiment of `text` is?"""

get_detailed_sentiment("I'ma Mario, and I'ma gonna wiiiiin!")
# {'sentiment_score': 0.8, 'summary_in_a_word': 'energetic'}
```

## But, types are not all you need

Again, type-safety is amazing. Providing guarantees on the output of the LLM helps a lot, but we think both of these didn’t go far enough. They left a few  questions unanswered:

1. **What is the full prompt?** *You can’t see it until you run the code with debug settings. Does updating the library break me?*
2. **How do you test?** *Do you copy pasting prompts and json blobs into OpenAI’s playground or into boilerplate pytest code?*
3. **How do you fail-over** to Anthropic when GPT4 goes down?
4. **How do you test against that other LLMs?** *Copy and paste or do you build an abstraction layer?*

Answering these questions requires more than just a python library. 

## Introducing BAML + The first VSCode LLM Playground

BAML is a lightweight programming language to define AI function interfaces, with a native VSCode extension.

Watch this 30-second video on how you can create and test AI functions without ever leaving VSCode.
(video coming soon!)


Here’s what a `.baml` AI function looks like (watch the video to see how the prompt works):

```rust
// example.baml
function GetDetailedSentiment {
    input string
    output DetailedSentiment
}

class DetailedSentiment {
    sentiment_score float
    summary_in_a_word string
}
```

BAML compiles to fully typed Python and TypeScript. No matter how you change the prompt, or the LLM model, or fail-overs, the python code doesn’t change — unless you change your AI function’s signature.

```python
# app.py
from baml_client import baml as b

async def main():	
    message = "I'ma Mario, and I'ma gonna wiiiiin!"

    # Your AI function defined in .baml files
    response = await b.GetDetailedSentiment(message)

    # Response is automatically strongly typed and
    # works with auto complete!
    print(f"Score: {response.sentiment_score}")
    print(f"Summary: {response.summary_in_a_word}")
```

## Getting Started

Start by [installing BAML](/v3/home/installation) and reading our [Hello World Tutorial](/v3/guides/hello_world/level0). 

Learning a new language seems daunting, but it takes < 10 minutes to get started. 

The VSCode extension provides auto-compiling on save, a realtime preview of the full prompt, syntax highlighting and great errors — every syntax error recommends a fix. 

Making BAML easy to read and write is our core design philosophy.

#### What you get out-of-the-box
- **Typed Python/Typescript support**
- **VSCode Playground**: see the full prompt and run tests
- **Better code organization** — no scattered jinja templates or yaml files
- **Its fast!** BAML compiles into PY and TS in less than 50ms (We ❤️ Rust)
- **Full integration with** **Boundary Studio** - our observability dashboard
    - Turn live production data into a test-case with one click!
- **Get structured responses,** 11 natively supported  types, including custom classes
- **Hallucination Checks**, when LLMs return something unexpected, we throw an exception
- **Works with any LLM,** even your own

- And best of all, **everything lives in your codebase.**

    