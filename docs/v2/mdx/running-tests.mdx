---
title: "Running Tests"
---

# Using the playground
Use the playground to run tests against individual function impls.
<img src="/images/baml/baml-playground.png" />

# Using pytest -- more advanced cases

For python, you can leverage **pytest** to run tests. All you need is to add a **@baml_test** decorator to your test functions to get your test data visualized on the baml dashboard.

## Example

```python
from baml_client.testing import baml_test
# Import your baml-generated LLM functions
from baml_client import baml
# Import any custom types defined in .baml files
from baml_client.baml_types import Sentiment


@baml_test
async def test_happy_user():
    response = await baml.ClassifySentiment.get_impl("v1").run("I am ecstatic")
    assert response == Sentiment.POSITIVE


## Or use a test class
@baml_test
@pytest.mark.asyncio
class TestHappyUser:
    async def test_happy_user(self):
        response = await baml.ClassifySentiment.get_impl("v1").run("I am ecstatic")
        assert response == Sentiment.POSITIVE

    async def test_happy_user2(self):
        response = await baml.ClassifySentiment.get_impl("v1").run("I am happy")
        assert response == Sentiment.POSITIVE
```

<Warning>
  Make sure your test file, the Test class and/or test function is prefixed with
  `test` or `Test` respectively. Otherwise, pytest will not pick up your tests.
</Warning>



## Pre-requisites

#### Ensure you have baml python sdk installed

```bash
pip install baml
```

#### Setup environment variables

On the dashboard, click on a project you've created and go to "Keys". Note down the project ID and create a new secret to use as the `baml_APP_SECRET` below.

Create a .env file in your project root and add the following:

```bash .env
baml_APP_ID=proj_123...
baml_APP_SECRET=baml:your-key-from-dashboard
OPENAI_API_KEY=your-open-ai-key

## If using bamlConfig with Azure LLMs, you'll need to set the following as well:
OPENAI_API_TYPE=azure
OPENAI_API_VERSION=2023-03-15-preview # or another version
AZURE_BASE=https://your-instance.openai.azure.com/
OPENAI_API_BASE=https://...
```

You may need to load these environment variables everytime you run a test command.
You can install `dotenv` CLI to do this for you.

<CodeGroup>
```bash pip
pip install "python-dotenv[cli]"
```

```bash poetry
poetry add "python-dotenv[cli]"
```

</CodeGroup>

Now you can prefix all commands with `dotenv run pytest ...` or `pipenv run pytest ...` to load the environment variables.

### Running tests

<Note>
  You must have environment variables loaded correctly before running tests. See
  the previous section.
</Note>
<Note>
  Make sure you are running these commands from your python virtual environment
  (or **`poetry shell`** if you use poetry)
</Note>

```bash
# From your project root
# Runs all tests generated from @test_group
# For every function, for every variant
pytest -m baml_test
```

If this doesn't work try running:
`python -m pytest -m baml_test`

To run tests for a subdirectory

```bash
# From your project root
# Note the underscore at the end of the folder name
pytest -m baml_test ./your-tests-folder/
```

To run tests that have a specific name or group name

```bash
# From your project root
pytest -m baml_test -k test_group_name
```

You can read more about the `-k` arg of pytest here ([PyTest Docs](https://docs.pytest.org/en/latest/example/markers.html#using-k-expr-to-select-tests-based-on-their-name))

`-k` will match any tests with that given name.

To run a specific test case in a test group

```bash
# From your project root
pytest -m baml_test -k '[test_group_name] and [test_case_name]'
```

### Parameterized Tests

Parameterized tests allow you declare a list of inputs and expected outputs for a test case. baml will run the test for each input and compare the output to the expected output.

```python
from baml_client.testing import baml_test
# Import your baml-generated LLM functions
from baml_client import baml
# Import any custom types defined in .baml files
from baml_client.baml_types import Sentiment

@baml_test
@pytest.mark.parametrize(
    "input, expected_output",
    [
        ("I am ecstatic", Sentiment.POSITIVE),
        ("I am sad", Sentiment.NEGATIVE),
        ("I am angry", Sentiment.NEGATIVE),
    ],
)
async def test_sentiments(input, expected_output):
    response = await baml.ClassifySentiment.get_impl("v1").run(input)
    assert response == expected_output
```

This will generate 3 test_cases on the dashboard, one for each input.

### Using custom names for each test

The parametrize decorator also allows you to specify a custom name for each test case. See below on how we name each test case using the ids parameter.

```python
from baml_client.testing import baml_test
from baml_client import baml
from baml_client.baml_types import Sentiment

test_cases = [
    {"input": "I am ecstatic", "expected_output": Sentiment.POSITIVE, "id": "ecstatic-test"},
    {"input": "I am sad", "expected_output": Sentiment.NEGATIVE, "id": "sad-test"},
    {"input": "I am angry", "expected_output": Sentiment.NEGATIVE, "id": "angry-test"},
]

@baml_test
@pytest.mark.parametrize(
    "test_case",
    test_cases,
    ids=[case['id'] for case in test_cases]
)
# Note the argument name "test_case" is set by the first parameter in the parametrize() decorator
async def test_sentiments(test_case):
    response = await baml.ClassifySentiment.get_impl("v1").run(test_case["input"])
    assert response == test_case["expected_output"]
```

### Grouping Tests by input type

Alternatively, you can group things together logically by defining one test case or test class per input type your testing. In our case, we'll split up all Positive sentiments into their own group.

```python
from baml_client.testing import baml_test
# Import your baml-generated LLM functions
from baml_client import baml
# Import any custom types defined in .baml files
from baml_client.baml_types import Sentiment

@baml_test
@pytest.mark.asyncio
@pytest.mark.parametrize(
  # Note we only need to pass in one variable to the test, the "input".
  "input",
  [
      "I am ecstatic",
      "I am super happy!"
  ],
)
class TestHappySentiments:
    async def test_sentiments(input, expected_output):
        response = await baml.ClassifySentiment.get_impl("v1").run(input)
        assert response == Sentiment.POSITIVE

@baml_test
@pytest.mark.asyncio
@pytest.mark.parametrize(
  # Note we only need to pass in one variable to the test, the "input".
  "input",
  [
      "I am sad",
      "I am angry"
  ],
)
class TestSadSentiments:
    async def test_sentiments(input, expected_output):
        response = await baml.ClassifySentiment.get_impl("v1").run(input)
        assert response == Sentiment.NEGATIVE
```

Alternatively you can just write a test function for each input type.

```python
from baml_client.testing import baml_test
from baml_client import baml
from baml_client.baml_types import Sentiment

@baml_test
@pytest.mark.asyncio
@pytest.mark.parametrize(
    "input",
    [
        "I am ecstatic",
        "I am super happy!",
        "I am thrilled",
        "I am overjoyed",
    ],
)
async def test_happy_sentiments(input):
    response = await baml.ClassifySentiment.get_impl("v1").run(input)
    assert response == Sentiment.POSITIVE

@baml_test
@pytest.mark.asyncio
@pytest.mark.parametrize(
    "input",
    [
        "I am sad",
        "I am angry",
        "I am upset",
        "I am frustrated",
    ],
)
async def test_sad_sentiments(input):
    response = await baml.ClassifySentiment.get_impl("v1").run(input)
    assert response == Sentiment.NEGATIVE
```